
\chapter{基于异质特征的字典学习方法}

\section{字典学习}

学位论文的封面由研究生院按国家规定统一制定印刷，封面内容必须打印，不得手写。

\section{耦合字典学习}

（1）行间距：固定值~20~磅（题名页除外）。

（2）字符间距：标准。

（3）页眉设置：单面页码页眉标题为章节题目，每一章节的起始页必须在单面页码，双面页码页眉标题统一为“西安电子科技大学博/硕士学位论文”，页眉标题居中排列，字体为宋体，字号为五号。页眉文字下添加双横线，双横线宽度为~0.5~ 磅，距正文距离为：上下各~1~磅，左右各~4~磅。

（4）页码设置：学位论文的前置部分和主体部分分开设置页码，前置部分的页码用罗马数字标识，字体为~Times New Roman~，字号为小五号；主体部分的页码用阿拉伯数字标识，字体为宋体，字号为小五号。页码统一居于页面底端中部，不加任何修饰。

（5）页面设置：为了便于装订，要求每页纸的四周留有足够的空白边缘，其中页边距为上~3~厘米、下~2~厘米；内侧~2.5~厘米、外侧~2.5~厘米；装订线为~0.5~厘米；页眉~2~厘米，页脚~1.75~厘米。

\section{判别性字典学习}

The classical DL method minimizes the reconstruction error of the given set of data subject to a sparsity constraint. Let $\mathbf{X}=[\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_{N}] \in \mathbb{R}^{p \times N}$ be the data matrix, where $N$ is the number of data samples and $p$ is the dimensions of the data features. The dictionary composed of $K$ atoms can be learned by minimizing the following objective function:
\begin{flalign}\label{equ:dic_learn}
&\min_{\mathbf{D,A}}~~\big\|\mathbf{X}-\mathbf{DA}\big\|_{\mathrm{F}}^2+\lambda\|\mathbf{A}\|_1\\
&\mathrm{~s.t.} \quad \|\mathbf{d}_i\|\leq 1, ~\forall i\in [1:K] \nonumber
\end{flalign}
where \textcolor{blue}{$\mathbf{D}=[\mathbf{d}_1,\mathbf{d}_2,\cdots,\mathbf{d}_K]\in\mathbb{R}^{p\times K}$} is the learned dictionary, $\mathbf{d}_i$ is the $i$-th atom of the dictionary,
 $\mathbf{A}\in\mathbb{R}^{K\times N}$ contains the sparse coefficients of $\mathbf{X}$, and $\|\cdot\|_F$ denotes Frobenius norm.
Besides, $\ell_1$-norm $\|\cdot\|_1$ working as a penalty term encourages sparsity of the solved coefficients $\mathbf{A}$ \cite{tibshirani1996regression}, and $\lambda$ controls the sparsity.

For multi-modal data, the class label information is available and often important, but the classical DL methods assume that a data sample would be the linear combination of a few atoms in a dictionary. Such a dictionary could consist of both the relevant data samples and some redundant data samples from the dataset, and some noises would be imposed. The dictionary learned from classical methods cannot reflect the intrinsic structure and shared structure among the data from different classes. Therefore, discriminative DL methods \cite{mairal2008discriminative}, \cite{mairal2009supervised} are proposed to deal with the aforementioned problem, which not only discover the dictionary atoms associated with each class, but also exploit the discriminative ability hidden in the sparse coefficients.

Given the data matrix $\mathbf{X}$, each data sample $\mathbf{x}_i$ has only one of $C$ class labels. We denote the structured discriminative dictionary $\mathbf{D}$ as $\mathbf{D}=[\mathbf{D}_{(1)},\mathbf{D}_{(2)},\ldots,\mathbf{D}_{(C)}]\in\mathbb{R}^{p\times K}$ which consists of $C$ sub-dictionaries, where $C$ is the total number of classes and each sub-dictionary has the same dimension, \textcolor{blue}{\textit{i.e.} each $\mathbf{D}_{(c)} \in \mathbb{R}^{p \times (K/C)}$. And the discriminative dictionary atoms are in correspondence with the class labels.} The discriminative DL framework is formulated as follows:
\begin{flalign}\label{equ:dis-dic}
&\min_{\mathbf{D,A}}~ \sum_{c=1}^C \big\|\mathbf{X}_{(c)}-\mathbf{D}_{(c)} \mathbf{A}_{(c)}\big\|_{\mathrm{F}}^2+\lambda\|\mathbf{A}\|_1\\
&\mathrm{~s.t.} \quad \|\mathbf{d}_i\|\leq 1, ~\forall i\in [1:K] \nonumber
\end{flalign}
where $\mathbf{X}_{(c)}$ denotes the data sample with $c$-th label, $\mathbf{A}_{(c)}$ is the corresponding sparse coefficients, and \textcolor{blue}{$\mathbf{A}$ consists of $\{\mathbf{A}_{(c)}\}^{C}_{c=1}$.}
Given a specific class, the discriminative DL framework encourages reconstructing the data samples in this class with the in-class dictionary,
and meanwhile penalizes the reconstruction of the data samples using the out-of-class dictionaries \cite{shekhar2013generalized}.

\section{其他说明}

本规定由研究生院负责解释，从申请~2015~年~9~月毕业和授位的研究生开始执行，其它有关规定同时废止。研究生毕业论文撰写要求参照学位论文撰写要求执行。
