
\chapter{基于判别性特征空间学习的多模态检索方法}

我校研究生学位论文包括以下几个部分：

\section{引言}

（1）题目：题目是以最恰当、最简明的词语反映论文中最重要的特定内容的逻辑组合，力求简短切题。中文题目（包括副标题和标点符号）一般不超过~20~个字，英文题目一般不超过~10~个实词。

题目位于确定位置的文本框中，文本框格式为水平位置：相对于右侧页边距绝对位置~0.5~厘米；垂直位置：相对于下侧页边距绝对位置~15~厘米；文字环绕方式为浮于文字上方；文本框大小：高度为绝对值~3.2~厘米，宽度为绝对值~15~厘米。文字格式为中文宋体、英文~Times New Roman~，二号加粗，居中对齐，左右不缩进，段前段后不留空，行距为固定值~30~ 磅。

（2）责任者姓名：包括论文作者姓名、指导教师姓名及职称（博士学位论文、学术型和同等学力硕士学位论文）以及学校、企业导师姓名及职称（专业学位硕士学位论文）。没有企业导师的专业学位类别请将“企业导师姓名及职称”栏目删除。

（3）申请学位类别：按照学科门类和学位层次填写，如工学博士、工学硕士、工程硕士、工商管理硕士等。

作者姓名、指导教师姓名职称、申请学位类别信息位于确定位置的文本框中，文本框格式为水平位置：相对于右侧页边距绝对位置~3.5~厘米；垂直位置：相对于下侧页边距绝对位置~20~厘米；文字环绕方式为浮于文字上方；文本框大小：高度为绝对值~3.5~厘米，宽度为绝对值~9~厘米。标题字体为黑体四号加粗，具体内容的文字格式为中文宋体、英文~Times New Roman~，四号加粗，左对齐，左右不缩进，段前段后不留空，行距为固定值~30~ 磅。

\section{基于判别性字典的多模态检索}

\subsection{Introduction}
Due to the explosive growth of data on the Internet, there have been increasing interests in similarity search with large-scale datasets such as text, audio, images, and videos. However, most existing similarity search methods only apply to the \emph{uni-modal} data setting, which refers to retrieve similar data items with a single modality to a query item. Nowadays, a more appealing scenario frequently arises in similarity search applications, which involves data with multiple modalities and is thus regarded as \textit{cross-modal} problems. Taking multimedia retrieval as an example, one would like to find some textual information that can well describe an image or video~\cite{wu2012heterogeneous}. Since data with different modalities reside in different feature spaces, the main challenging factor pertaining to cross-modal problems is how to capture and correlate heterogeneous features from different modalities.

Several promising approaches have been proposed to deal with cross-modal retrieval. Canonical Correlation Analysis (CCA) and its variants may be the most popular techniques that are widely used in cross-modal retrieval. Specifically, given visual and textual features, CCA maps the two modalities into a common latent space where the feature correlation between the two modalities is maximized~\cite{rasiwasia2010new}. As a supervised extension of CCA, Generalized Multiview Analysis (GMA)~\cite{sharma2012generalized} is proposed for cross-modal retrieval. %However, existing CCA-based approaches make a strong assumption that different modalities have a common or shared subspace, which is restricted to some extent for cross-modal retrieval in real-world settings.

Another kind of methods are based on Latent Dirichlet Allocation (LDA). Following the seminal work of Blei \emph{et al.}~\cite{blei2003lda}, two association models, \textit{i.e.}, correspondence LDA (corr-LDA)~\cite{blei2003modeling} and Topic-regression multi-modal LDA (tr-mmLDA)~\cite{putthividhy2010topic}
extended from the basic LDA model, have been built to learn joint distributions of multi-modal data collections. In addition, the Hierarchical Dirichlet Process (HDP) model automatically learns some interpretable topics describing shared and private structure~\cite{virtanen2012factorized}. %However, these extensions endeavor to enforce strong correlations among different modalities even though they do not exist.

%As we mentioned in the above, since heterogeneous data resides in different feature spaces, how to model the relationship across these modalities becomes a prominent issue that needs to be addressed. With the goal of transferring knowledge from the source domain to the target domain, recent studies in transfer learning have shown promising results for cross-domain recognition tasks. Among techniques for tacking cross-domain recognition, the most used efficient way is to determine a common feature space or a compact representation by leveraging cross-domain unlabeled data pairs from both domains~\cite{huang2013coupled}. Inspired by transfer learning, in this paper we also learn a common feature space to unify different modalities, where the relationship between images and their related textual information can be established. On the other hand, when class labels of multi-modal data are available, it is natural to assume that the intra-modality data within the same class shares some common aspects, whilst the inter-modality data within the same class has closer associations.

Although there exist some methods for solving the cross-modal retrieval problem, most of them focus on learning relevant features from two distinct feature spaces, and ignore the heterogeneous feature representation. Considering the fact that dictionary learning (DL) methods have the intrinsic power of dealing with heterogeneous features through generating particular dictionaries for different data modalities, Zhuang \emph{et al.} propose supervised coupled dictionary learning with group structures for multi-modal retrieval (SLiM$^{2}$)~\cite{zhuang2013supervised}, which jointly learns a group of mapping functions across different modalities. And a unified model is proposed for cross-domain image synthesis and recognition~\cite{huang2013coupled}, in which a pair of dictionaries for two domains are learned, and multi-modal data is then mapped to a common feature space that captures and correlates heterogeneous data. Nevertheless, these methods overlook exploring the discrimination of multi-modal data from different classes.

To solve the challenges above, we propose a discriminative latent feature space learning model for cross-modal retrieval, where discriminative dictionaries and a group of mapping functions across different modalities are jointly learned to produce a common feature space. Concretely, we first learn a class-specific discriminative dictionary for each modality, and discover the discrimination of the intra-modality data in different classes when incorporating class label information. Afterwards, we map a pair of sparse codes, obtained over the learned dictionaries, to the common feature space where the relevance property of the inter-modality data in the same class can also be discovered.
%Moreover, the learned common feature space can be utilized to update the observed dictionary pair to further boost the representation power of data in each modality.
Experimental results on two public cross-modal datasets show that the proposed method outperforms several state-of-the-art methods.

%Main contributions of our work can be summarized as follows:
%\begin{itemize}
%  \item We propose a generalized model that jointly learn class-specific discriminative dictionary and latent feature space for cross-modal retrieval, which suggests that pair of dictionary coefficient is related by projecting them to the common feature space.
%  \item  We utilize available label information to boost not only the discriminativeness of intra-modality data from the same classes but also the relevance of inter-modality data from the same classes, which is beneficial to cross-modal tasks.
%  \item Experimental results on two popular cross-modal datasets have shown that our proposed framework outperforms several state-of-the-art methods.
%\end{itemize}

%The remainder of this paper is organized as follows. In Section 2, we propose the discriminative latent feature space learning model for cross-modal retrieval, as well as an iterative algorithm for solving the related optimization problem. In Section 3, we present experimental results on two public cross-modal datasets. Finally, we conclude the paper in Section 4.

\subsection{The Proposed Method }
\label{sec:method}
%In section 2.1, we first  present the discriminative dictionary learning, which is the fundamental technique in our work. We describe the problem formulation and explain how to represent and associate cross-modal data by jointly solving discriminative dictionary and latent common feature space learning problems in section 2.2. Optimization details for the training stage of our model are presented in section 2.3. Finally, we detail the procedure of cross-modal retrieval.

\subsubsection{Discriminative Dictionary Learning}
\label{subsec:DDL}
%DL methods have the intrinsic power of dealing with the heterogeneous features by generating different dictionaries for %multi-modal data, which has been used in several fields such as face recognition \cite{wright2009robust}, super-resolution %\cite{wang2012semicoupled}, photo-sketch synthesis \cite{huang2013coupled} and cross-modal retrieval~\cite{zhuang2013supervised}.

%The classical DL method minimizes the reconstruction error of the given set of data subject to a sparsity constraint. Let $\mathbf{X}=[\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_{N}] \in \mathbb{R}^{p \times N}$ be the data matrix, where $n$ is the number of data and $p$ is the dimensions of the data feature. The $K$ atoms dictionary can be learned by minimizing the following question:
%\begin{equation}\label{equ:dic_learn}
%\begin{split}
%&\min_{\mathbf{D,A}}\|\mathbf{X}-\mathbf{DA}\|_F^2+\lambda\|\mathbf{A}\|_1\\
%&s.t. \qquad \|\mathbf{d}_i\|\leq1, \quad \forall i,\\
%\end{split}
%\end{equation}
%where $\mathbf{D}=[\mathbf{d}_1,\mathbf{d}_2,...,\mathbf{d}_K]$ is the learned dictionary and $\mathbf{A}$ is the sparse coefficient of $\mathbf{X}$.

The classical DL method minimizes the reconstruction error of the given set of data subject to a sparsity constraint. For multi-modal data, however, the class label information is available and important usually, and the dictionary learned in classical methods cannot reflect the intrinsic structure and shared structure among the data from different classes. Therefore, discriminative DL \cite{mairal2008discriminative} \cite{mairal2009supervised} is proposed to deal with this problem, which not only discover the dictionary atoms associated with each class, but also exploit the discriminative ability hidden in the sparse coefficients.

Given the data $\mathbf{X}$, each data object $\mathbf{x}_i$ has only one of $C$ class labels. We denote the discriminative dictionary $\mathbf{D}$ and the dictionary atom $\mathbf{d}_i$ is in correspondence with only one class label. The discriminative DL is formulated as:
\begin{equation}\label{equ:dis-dic}
\begin{split}
&\min_{\mathbf{D,A}} \|\mathbf{X}-\mathbf{DA}\|_F^2+\lambda\|\mathbf{A}\|_1\\
& \qquad \quad +\|\mathbf{X}-\mathbf{D}\mathbf{A}_{in}\|_F^2 + \|\mathbf{DA}_{out}\|_F^2\\
\end{split}
\end{equation}
where matrices $\mathbf{A}_{in}$ and $\mathbf{A}_{out}$ are given as:
\begin{displaymath}
  \mathbf{A}_{in} (i,j)=\left\{
\begin{array}{ll}
\mathbf{A} (i,j), &\mathbf{d}_i,\mathbf{x}_j \in \textrm{same class}\\
0, &\textrm{otherwise},
\end{array}
\right.
\end{displaymath}

\begin{displaymath}
  \mathbf{A}_{out} (i,j)=\left\{
\begin{array}{ll}
\mathbf{A} (i,j), &\mathbf{d}_i,\mathbf{x}_j \in \textrm{different class}\\
0,&\textrm{otherwise}.
\end{array}
\right.
\end{displaymath}
%Given a class, the discriminative DL encourages to reconstruct data with in-class dictionary, and penalizes reconstruction with out-of-class dictionaries. %\cite{shekhar2013generalized}.
\subsubsection{Problem Formulation}
\label{subsec:formulation}
%For simplify we assume that there are two modalities $\mathcal{M}=\{m_1,m_2\}$, eg. $text$ and $image$.
Suppose that there are $s$ modalities $\mathcal{M}=\{m_1,m_2,...m_s\}$, e.g. $text$ and $image$, etc. Let $\mathbf{X}_m=[\mathbf{x}_{m,1},\mathbf{x}_{m,2},\ldots,\mathbf{x}_{m,N}]\in \mathbb{R}^{p_m\times{N}}$ be the data matrices from $m\in \mathcal{M}$ modality. Each data object pair $\{\mathbf{x}_{m_1,i}, \mathbf{x}_{m_2,i},\ldots,\mathbf{x}_{m_s,i} \}$ belongs only one of $C$ classes and describe the same semantic content. $N$ denotes the number of data object pairs, and $p_m$ denotes the dimensionality of $m$ modality.

Our model aim to obtain the sparse coefficients of data from different modalities, and to map them to a discriminative common feature space, where the data share common representation. The modal-specific dictionaries and mapping matrices are learned and coupled, which can bridge across different modalities by the latent common space. Moreover, the discriminative dictionary ensure the ability of separating different classes.

Assume that $\mathbf{D}_m=[\mathbf{D}_{m(1)},\mathbf{D}_{m(2)},\ldots,\mathbf{D}_{m(C)}]\in \mathbb{R}^{{p_m}\times{K}}$ is the discriminative dictionary from $\mathbf{X}_m$ and $\mathbf{A}_m\in \mathbb{R}^{K \times N}$ is its corresponding sparse coefficients, where $K$ is the size of the dictionary. $\mathbf{X}_{m(c)}$ is denoted as the data $\mathbf{X}_{m}$ with $c$-th label and $\mathbf{A}_{m(c)}$ is the sparse coefficient corresponding to $\mathbf{X}_{m(c)}$ and $\mathbf{D}_{m(c)}$. The learning cross-modal discriminative DL is formulated as follows:
\begin{equation}\label{equ:object}
\begin{split}
&\min_{\mathbf{D}_m,\mathbf{A}_m,\mathbf{U}_m}\sum\limits_{m\in \mathcal{M}}{\sum\limits_{c=1}^{C}{\left\| {\mathbf{X}_{m(c)}}-{\mathbf{D}_{m(c)}}{\mathbf{A}_{m(c)}} \right\|_{F}^{2}}}\\
&\qquad  +\gamma\sum\limits_{m \in \mathcal{M}}{\sum\limits_{n\ne m}^{{}}{\left\| {\mathbf{U}_{m}}{\mathbf{A}_{m}}-{\mathbf{U}_{n}}{\mathbf{A}_{n}} \right\|_{F}^{2}}}\\
&\qquad +\sum\limits_{m\in \mathcal{M}}{{\lambda_{m}}{\left\| {\mathbf{A}_{m}} \right\|}_{1}}+\mu\sum\limits_{m \in \mathcal{M}}{{\left\| \mathbf{U}_{m} \right\|}_{F}}\\
&\qquad  s.t. \qquad \|\mathbf{d}_{m,i}\|_2\leq1 , \quad \forall m,i.\\
\end{split}
\end{equation}
In Eq. \ref{equ:object}, $\gamma$, $\lambda_m$ and $\mu$ are regularization parameters. $\mathbf{U}_m\in\mathbb{R}^{K \times K}$ is the mapping matrix for $\mathbf{A}_m$ and $\mathbf{U}_m\mathbf{A}_m\in\mathbb{R}^{K \times N_m}$ is the mapped data of $\mathbf{X}_m$ in the $K$ dimensional discriminative common feature space. %The first term $\left\| {\mathbf{X}_{m(c)}}-{\mathbf{D}_{m(c)}}{\mathbf{A}_{m(c)}} \right\|_{F}^{2}$ minimizes the reconstruction error for each class, which guarantees the discriminativeness in the sparse representations, and the second term $\left\| {\mathbf{U}_{m}}{\mathbf{A}_{m}}-{\mathbf{U}_{n}}{\mathbf{A}_{n}} \right\|_{F}^{2}$ minimizes the mapping error in the discriminative common feature space. The term ${\left\|\mathbf{A}_m\right\|}_1$ enforces the sparsity and the term ${\left\| \mathbf{U}_{m} \right\|}_{F}$ is imposed to avoid over-fitting. It is worth noting that our modal further relaxes the assumption in SLIM$^2$ \cite{zhuang2013supervised} which is a special case of our model when $\mathbf{U}_m=\mathbf{I}$. On the other hand, different from \cite{huang2013coupled}, it is not necessary to keep the mapping matrices $\mathbf{U}_m$ and $\mathbf{U}_n$ reversible in our model. After mapping the sparse coefficients, the discriminative features from the different classes inside intra-modality and the relevant features from the same classes in inter-modality co-occur in the common feature space.


\subsubsection{Optimization}
\label{subsec:optimization}
%\renewcommand{\algorithmicrequire}{\textbf{Input:}} % Use Input in the format of Algorithm
%\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm
%\begin{algorithm}[!t]
%\caption{The Iterative Algorithm for Our Model}
%\label{alg:optimization}
%\begin{algorithmic}[1]
%\Require
%Data matrices $\mathbf{X}_m$ with $C$ labels from different modalities in $\mathcal{M}$.
%\Ensure
%Dictionaries $\mathbf{D}_m$, and mapping matrices $\mathbf{U}_m$ of data from different modalities in $\mathcal{M}$.
%\State Initialize $\mathbf{D}^{(0)}_m$ and $\mathbf{A}^{(0)}_m$ by FDDL and set $\mathbf{U}^{(0)}_m$ as $\mathbf{I}$.
%\While {not converge}
%\For {$k=1,2,...,C$}
%\State Fix other variables and update $\mathbf{D}^{(i+1)}_{m(k)}$ by Eq. \ref{equ:update-d} with $\mathbf{A}^{(i)}_{m(k)}$ and $\mathbf{U}^{(i)}_{m(k)} $.
%\EndFor
%\State Fix other variables and update $\mathbf{A}^{(i+1)}_{m}$ by Eq. \ref{equ:update-a} with $\mathbf{D}^{(i)}_{m}$ and $\mathbf{U}^{(i)}_{m} $.
%\State Fix other variables and update $\mathbf{U}^{(i+1)}_{m}$ by Eq. \ref{equ:update-u} with $\mathbf{D}^{(i)}_{m}$, $\mathbf{A}^{(i)}_{m} $ and $\mathbf{U}^{(i)}_{n}$.
%\EndWhile
%\end{algorithmic}
%\end{algorithm}

Although it is difficult to optimize all the variables simultaneously, the objective function in Eq. \ref{equ:object} is convex to each of variables when others are fixed. Thus we conduct a iterative algorithm to optimize the variables,  respectively.

Firstly, we update $\mathbf{D}_m$. By applying joint dictionary learning, $\mathbf{D}_m$ is calculated for initializing the optimization procedure. Here, the Fisher Discrimination Dictionary Learning (FDDL) scheme \cite{yang2011fisher} is applied for the initialization. We consider $\mathbf{A}_m$ and $\mathbf{U}_m$ as constants when updating the dictionaries during each iteration. Therefore, the dictionary $\mathbf{D}_m$ can be updated as follows:
\begin{equation}\label{equ:update-d}
\begin{split}
&\min_{\mathbf{D}_{m}}\sum\limits_{m\in \mathcal{M}}{\sum\limits_{c=1}^{C}{\left\| {\mathbf{X}_{m(c)}}-{\mathbf{D}_{m(c)}}{\mathbf{A}_{m(c)}} \right\|_{F}^{2}}}\\
&\qquad s.t. \|\mathbf{d}_{m,i}\|_2\leq1, \quad \forall i,
\end{split}
\end{equation}
which is a quadratically constrained quadratic program (QCQP) problem with respect to $\mathbf{D}_{m(c)}$ which can be updated respectively.

Next, the sparse coefficients $\mathbf{A}_m$ are we calculated when the mapping matrices $\mathbf{U}_m$ and dictionaries $\mathbf{D}_m$ are fixed. As a result, the Eq. \ref{equ:object} is converted into the following formula:
\begin{equation}\label{equ:update-a0}
\begin{split}
&\min_{\mathbf{A}_m} \sum\limits_{m\in \mathcal{M}}{\left\| {\mathbf{X}_{m}}-{\mathbf{D}_{m}}{\mathbf{A}_{m}} \right\|_{F}^{2}}\\
&+\gamma\sum\limits_{m \in \mathcal{M}}{\sum\limits_{n\ne m}^{{}}{\left\| {\mathbf{U}_{m}}{\mathbf{A}_{m}}-{\mathbf{U}_{n}}{\mathbf{A}_{n}} \right\|_{F}^{2}}}+\sum\limits_{m\in \mathcal{M}}{{\lambda_{m}}{\left\| {\mathbf{A}_{m}} \right\|}_{1}}
\end{split}
\end{equation}

For the Eq. \ref{equ:update-a0}, the first two terms can be combined as one term:
\begin{equation}\label{equ:update-a}
\begin{split}
&\min_{\mathbf{A}_m} \sum\limits_{m\in \mathcal{M}}{\left\| {\mathbf{\hat{X}}_{m}}-{\mathbf{\hat{D}}_{m}}{\mathbf{A}_{m}} \right\|_{F}^{2}}+\sum\limits_{m\in \mathcal{M}}{{\lambda_{m}}{\left\| {\mathbf{A}_{m}} \right\|}_{1}}
\end{split}
\end{equation}
where
$\mathbf{\hat{X}}_{m}= \left[\begin{array}{c} \mathbf{X}_m \\ \sqrt{\gamma} \mathbf{U}_n \mathbf{A}_n \\ \end{array} \right]$ and $\mathbf{\hat{D}}_m=\left[\begin{array}{c} \mathbf{D}_m \\ \sqrt{\gamma} \mathbf{U}_m \end{array} \right]$.
Then the Eq. \ref{equ:update-a0} is converted into standard sparse coding problem for each modal, which can be solved by SPAMS Toolbox~\cite{mairal2009online}.

Finally, we update the mapping matrices $\mathbf{U}_m$. When $\mathbf{D}_m$ and $\mathbf{A}_m$ are fixed, Eq. \ref{equ:object} becomes a ridge regression problem and we can solve it as below:
\begin{equation}\label{equ:update-u0}
\min_{\mathbf{U}_{m}} \gamma \sum\limits_{m \in \mathcal{M}}{\sum\limits_{n\ne m}{ \| \mathbf{U}_{m}\mathbf{A}_{m} -\mathbf{U}_{n}\mathbf{A}_{n} \|^{2}_{F}}} + \mu \|\mathbf{U}_{m}\|^{2}_{F}.
\end{equation}
We can obtain the analytical solutions of $\mathbf{U}_m$ in Eq. \ref{equ:update-u0} by:
\begin{equation}\label{equ:update-u}
\mathbf{U}_{m} = \mathbf{U}_n \mathbf{A}_n \mathbf{A}_m^{T}(\mathbf{A}_m \mathbf{A}_m^{T}+(\mu/\gamma)\mathbf{I} )^{-1}  \end{equation}
%It is noted that for the mapping matrices, the solution of $\mathbf{U}_m$ is not unique and one trivial solution is $\mathbf{U}_m= \mathbf{0}$. Therefore, as confirmed in \cite{huang2013coupled}, we add small perturbations to avoid to obtain the trivial solution.

%Algorithm \ref{alg:optimization} describes the details of optimization procedure of our proposed model.
After the optimization is completed, we can apply our derived model for cross-domain retrieval task.


\subsubsection{Cross-modal Retrieval}
\label{subsec:retrieval}
After the dictionary $\mathbf{D}_m$ and the mapping matrix $\mathbf{U}_m$ are learned, we can map all data of modality $m \in \mathbb{M}$ into the common
space. Given a data object $q_m$ from modality $m$, its sparse coefficients $a_m$ can be achieved by:
\begin{equation}\label{equ:obtain_a_m}
\min_{a_m}{\|q_m-\mathbf{D}_m a_m\|_F^2+\lambda_m\|a_m\|_1}
\end{equation}
Thus, the representation of $p_m$ in the common space is generated by:
\begin{equation}\label{equ:obtain_p_m}
p_m = \mathbf{U}_m a_m
\end{equation}
Finally, all data objects of modality are mapped in into the common space and the top $N$ nearest neighbours of $p_m$ are returned as results. %The cross-modal retrieval algorithm with our modal is shown in Algorithm \ref{alg:retrieval}.

%\begin{algorithm}[t]
%\caption{Cross-Modal Retrieval with Our Model}
%\label{alg:retrieval}
%\begin{algorithmic}[1]
%\Require
%Query data $q_m$ from modality $m$; the learned dictionaries $\mathbf{D}_m$ and $\mathbf{D}_n$; the learned mapping matrices $\mathbf{U}_m$ and $\mathbf{U}_n$; the dataset $\mathbf{X}_n$ to be retrieved from modality $n$.
%\Ensure
%The data retrieved in modality $n$.
%\State Initialize: compute the sparse coefficients $\mathbf{A}_n$ of $\mathbf{X}_n$ by Eq. \ref{equ:obtain_A_n}, map $\mathbf{A}_n$ into the common space by Eq. \ref{equ:obtain_P_n} with $\mathbf{U}_n$.
%\State Obtain the sparse coefficients $a_m$ of $q_m$ by Eq. \ref{equ:obtain_a_m}.
%\State Obtain the representation $p_m$ of $q_m$ in the common space by Eq. \ref{equ:obtain_p_m} with $\mathbf{U}_m$.
%\State Compute the Euclidean distance matrix between $p_m$ and $\mathbf{P}_n$
%\State The top $N$ nearest neighbours of the query $q_n$ are returned as final result.
%\end{algorithmic}
%\end{algorithm}


\subsection{Experiments}
\label{sec:experiments}
%In this section, we evaluate the performance of our proposed method for cross-modal retrieval. We first introduce datasets and evaluation metrics we adopted in this paper, and then elaborate compared methods and parameter tuning in our experiments. At last, we compare our approach with some representative state-of-the-art methods.
\subsubsection{Datasets}
\label{subsec:datasets}
\textbf{Wiki Text-Image.} The Wiki Text-Image dataset consists of 2,173/693(training/testing) image-text pairs which are from Wikipedia. All pairs are labeled by 10 classes. We extract SIFT descriptors for images and quantize them into Bag-of-Visual-Words (BoVW). For texts, we represent them with
Bag-of-Words (BoW) by counting the word frequency. Finally we obtain two datasets: one with 500 dimensions BoVW and 1,000 dimensions BoW, and the other with 1,000 dimensions BoVW and 5,000 dimensions BoW.

\textbf{NUS-WIDE.} The NUS-WIDE dataset is a web image dataset which contains images and the associated tags. We select image-tags pairs with only one class and pick up the top 10 classes. We use the 500 dimensions BoVW and 1000 dimension tags feature and random take 3\% and 1\% each class for training set and testing set, respectively.

\begin{table*}[t!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\multirow{3}{*}{Methods} & \multicolumn{3}{c|}{500-D BoVW \& 1000-D BoW} & \multicolumn{3}{c|}{1000-D BoVW \& 5000-D BoW}\\
\cline{2-7}
& \multicolumn{3}{c|}{mAP} & \multicolumn{3}{c|}{mAP}\\
\cline{2-7}
 & Image query& Text query& Average & Image query& Text query& Average\\
\hline
CCA  & 0.1650	& 0.1883 &	0.1766 & 0.1733 & 0.1822 & 0.1777 \\
GMLDA & 0.1701	& 0.2161 &	0.1931 & 0.1641 & 0.2550 & 0.2096 \\
GMMFA & 0.1721	& 0.2180 &	0.1951 & 0.1641 & \textbf{0.2761} & 0.2201\\
SLIM$^2$ & 0.1795	& 0.2123 &	0.1959 & 0.2169 & 0.2365 & 0.2267 \\
Proposed & \textbf{0.2174} & \textbf{0.2494} & \textbf{0.2334} & \textbf{0.2282} & 0.2606 & \textbf{0.2444}\\
\hline
\end{tabular}
\caption{Comparison of mAP performance of different methods on the Wiki dataset, with the best results marked by bold font.}
\label{tab:wiki}
\end{table*}

\subsubsection{Experimental Settings}
\label{subsec:setting}
%We evaluate our method on two public datasets, \emph{i.e.}, Wiki Text-Image dataset \cite{rasiwasia2010new} and the NUS-WIDE dataset \cite{chua2009nus}.





\begin{table}[t]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\multirow{2}{*}{Methods} & \multicolumn{3}{c|}{mAP} \\
\cline{2-4}
 & Image query& Text query& Average\\
\hline
CCA & 0.4345 & 0.3131 & 0.3738 \\
%PLS & 0.4431 & 0.3833 & 0.4132 \\
%BLM	& 0.4487 & 0.4229 & 0.4358 \\
GMLDA &	0.4396 & 0.3166	& 0.3781 \\
GMMFA &	0.4416 & 0.3169	& 0.3792 \\
SLIM$^2$ &	0.4271 & 0.4162	& 0.4216 \\
Proposed & \textbf{0.4640} & \textbf{0.4344} & \textbf{0.4492} \\
\hline
\end{tabular}
\caption{Comparison of mAP performance of different methods on the NUS-WIDE dataset, with the best results marked by bold font.}\label{tab:nuswide}
\end{table}

%\begin{table}[!t]
%\centering
%\begin{tabular}{|c|c|c|c|}
%\hline
%\multirow{2}{*}{Methods} & \multicolumn{3}{c|}{mAP} \\
%\cline{2-4}
% & Image query& Text query& Average\\
%\hline
%CCA  & 0.1650	& 0.1883 &	0.1766\\
%%PLS  & 0.1900	& 0.2280 &	0.2090\\
%%BLM  & 0.2041	& 0.2342 &	0.2191\\
%GMLDA & 0.1701	& 0.2161 &	0.1931\\
%GMMFA & 0.1721	& 0.2180 &	0.1951\\
%SLIM$^2$ & 0.1795	& 0.2123 &	0.1959\\
%Proposed & \textbf{0.2174} & \textbf{0.2494} & \textbf{0.2334}\\
%\hline
%\end{tabular}
%\caption{Comparison of mAP performance of different methods on the Wiki dataset (image feature is 500-D BoVW and text feature is 1000-D BoW), for image query text task, text query image task and the average of two tasks, with the best results marked by bold font.}
%\label{tab:wiki-small}
%\end{table}
%
%\begin{table}[!t]
%\centering
%\begin{tabular}{|c|c|c|c|}
%\hline
%\multirow{2}{*}{Methods} & \multicolumn{3}{c|}{mAP} \\
%\cline{2-4}
% & Image query& Text query& Average\\
%\hline
%CCA	 & 0.1733 & 0.1822 & 0.1777 \\
%%PLS & 0.1765 & 0.2219 & 0.1992 \\
%%BLM & 0.1988 & 0.2356 & 0.2172 \\
%GMLDA & 0.1641 & 0.2550 & 0.2096 \\
%GMMFA & 0.1641 & \textbf{0.2761} & 0.2201 \\
%SLIM$^2$ & 0.2169 & 0.2365 & 0.2267 \\
%Proposed & \textbf{0.2282} & 0.2606 & \textbf{0.2444}\\
%\hline
%\end{tabular}
%\caption{Comparison of mAP performance of different methods on the Wiki dataset (image feature is 1000-D BoVW and text feature is 5000-D BoW), for image query text task, text query image task and the average of two tasks, with the best results marked by bold font.}
%\label{tab:wiki-large}
%\end{table}




The retrieval performance is evaluated by mean Average Precision (mAP). The Average Precision (AP) of a set of $N$ returned data is defined by
% \begin{equation}
$   AP=\frac{1}{T}\sum^{N}_{r=1}P(r)\delta(r)$,
% \end{equation}
 where $T$ is the number of relevant data (\emph{i.e.}, data with the same labels) in the retrieved set, $P(r)$ denotes the precision of the top $r$ retrieved data, and $\delta(r)=1$ if the $r$th retrieved data is relevant and $\delta(r)=0$ otherwise. We then average the AP values over all the queries in the query set to obtain the mAP measure. Here we set $N=50$ as \cite{zhuang2013supervised}.

For a cross-modal retrieval problem, empirical studies are conducted for image-query-text task and text-query-image task. And we also calculate the average of two tasks for general evaluation. The parameters $\gamma$, $\lambda_{text}$, $\lambda_{image}$ and $\mu$ are set to 1, 0.01, 0.1, 0.1 on Wiki dataset and 0.01, 0.01, 0.01, 1 on NUS-WIDE dataset, respectively. Beside, we set the size of the dictionary $K=200$ empirically.

%\subsection{Compared Methods}
%\label{subsec:comparison}


%\begin{itemize}
%  \item CCA is the classic cross-modal retrieval method which maximize the correlation between the two modalities;
%  \item Partial Least Squares (PLS) and Bilinear Model (BLM) both learn common space for cross-modal document retrieval and face recognition;
%  \item GMMFA and GMLDA are based on the Generalized Multiview Analysis(GMA) model, which are the extensions of Marginal Fisher Analysis (MFA) and Linear Discriminant Analysis (LDA), respectively;
%  \item SLiM$^2$ is the supervised semi-coupled DL method for cross-modal retrieval.
%\end{itemize}

\begin{figure*}[t]
\centering
\includegraphics[width=0.85\linewidth]{paper-fig.pdf}
%\makeatletter\def\@captype{figure}\makeatother
\caption{A cross-modal retrieval example by our method. The top row is image-query-text task and the bottom row is text-query-image task.}
\label{fig:paper-fig}
\end{figure*}

%\subsection{Parameter Tuning}
%\label{subsec:tuning}
%For fair comparison, similar parameters are verified in our experiment.



\subsection{Performance Comparisons}
\label{subsec:performance}
We evaluate the performance of our proposed method, and compare it with several related methods: (1) CCA \cite{hardoon2004canonical}, (2) GMMFA \cite{sharma2012generalized}, (3) GMLDA \cite{sharma2012generalized}, and (4) SLiM$^2$ \cite{zhuang2013supervised} on Wiki Text-Image dataset and NUS-WIDE dataset. All mAP results on two datasets in our experiment are shown in Table \ref{tab:wiki} and Table \ref{tab:nuswide}, respectively. As we see, the proposed method achieves the almost overall best performance over the datasets.

For Wiki dataset, Table \ref{tab:wiki} shows the mAP scores of different methods on the two kinds of Wiki datasets. In terms of the average score of two task, our method achieves best mAP scores of 0.2334 and 0.2444 for the two different dimensions of datesets, respectively.

The comparison of mAP results on NUS-WIDE dataset are shown in Table \ref{tab:nuswide}. As shown in Table \ref{tab:nuswide}, We see that our method outperforms other methods, which proves that our proposed method is effective.

Figure~\ref{fig:paper-fig} shows an example of image-query-text task and text-query-image task on Wiki dataset, respectively. In order to visualize the results, we show the corresponding images to the top texts retrieved  image-query-text task. Note that the results returned by our method are semantically related.


\subsection{Conclusions}
\label{sec:conclusion}
In this paper, we proposed a general cross-modal retrieval framework, where discriminative dictionaries and a common feature space incorporating label information are learned jointly. The learned common space yields not only discriminative features for different classes inside the intra-modality data, but also similar features for the same class inside the inter-modality data. Therefore, the common feature space can capture and correlate cross-modal data, where cross-modal retrieval is conducted. Experiments conducted on two public cross-modal datasets show that the proposed model works better than competing methods.

\section{基于耦合字典学习与类标对齐的多模态检索}

\subsection{Introduction}
Cross-modal retrieval has attracted extensively attention due to the explosive growth of multimedia data, such as image, video and text. Given a text query, the task of cross-modal retrieval is to search the most relevant images from an image dataset, and vice versa. Different from the unimodal case, the data from different modalities reside in different feature space, and exist a natural semantic gap between them. Hence, the challenging problem of cross-modal retrieval is that how to sufficiently represent the heterogeneous data and effectively narrow down the semantic gap.

Recently, several cross-modal retrieval approaches have been proposed from different perspectives. As a classic technique, Canonical Correlation Analysis (CCA) \cite{hardoon2004canonical} is widely used in cross-modal retrieval task, which aims to obtain the maximally correlated subspace between two different modalities. Rasiwasia \textit{et al.} \cite{rasiwasia2010new} proposed to map the text and image from their original spaces to a CCA space. The work of \cite{sharma2012generalized} used a bilinear model (BLM) to learn a common space for cross-modal retrieval. Sharma \textit{et al.} \cite{sharma2011bypassing}  and Chen \textit{et al.} \cite{chen2012continuum} utilized Partial Least Squares (PLS) for cross-modal retrieval by linearly mapping data in different modalities into the common space. Besides CCA, BLM and PLS, Sharma \textit{et al.} \cite{sharma2012generalized} proposed Generalised Multiview Anlysis to extend Linear Discriminant Analysis (LDA) \cite{blei2003lda} and Marginal Fisher Analysis (MFA), named Generalized Multiview LDA (GMLDA) and Generalized Multiview MFA (GMMFA) for cross-modal retrieval. It is worth noting that, the foregoing methods always assume that both modalities can be projected into a common feature space where the maximized correlation is explored to narrow down the semantic gap.

On the other hand, to sufficiently represent the heterogeneous data resided in different feature spaces, dictionary learning has absorbed ever-increasing attention in recent years. Dictionary learning considers that a data sample can be reconstructed with a linear combination of a few atoms in a learned dictionary. Due to the intrinsic power of representing the heterogeneous features by generating different dictionaries for multi-modal data, dictionary learning technique has been adopted to deal with cross-style problems such as photo-sketch synthesis \cite{huang2013coupled} and super-resolution \cite{wang2012semicoupled}. Huang \textit{et al.} \cite{huang2013coupled} proposed a coupled dictionary learning (CDL) for cross-domain image recognition, where the different sparse coefficients are mapped into a common feature space. Recently Wu \textit{et al.} \cite{zhuang2013supervised} proposed coupled dictionary learning with group structure information for multi-modal retrieval (SLiM$^2$). Nevertheless, these methods also have very strict assumptions, which make them unsuitable for real multi-modal data and incapable to capture the inherent semantic relationship.

To cope with the aforementioned problems, this paper presents a novel cross-modal retrieval framework based on coupled dictionary learning with common label alignment. Specifically, we first separately obtain sparse coefficients to represent the heterogeneous features from different modalities by imposing dictionary learning into our model. Then, the data samples from different modalities are projected into a common space where the inherent relation
between modalities can be well discovered. Moreover, label information is leveraged to align the cross-modal data sample pairs in the common space so as to encourage the inherent correlation across the different modalities. Our model minimizes representation error and label space projection error simultaneously. To solve the framework efficiently, an iterative optimization strategy is exploited. Experimental results on two public datasets show that the proposed model outperforms several state-of-the-art methods.


\subsection{The Proposed Model }
\label{sec:method}
In this section, we begin with a brief introduction of dictionary learning. Then we formulate the objective function and its corresponding optimization algorithm.


\subsubsection{Dictionary Learning}
\label{subsec:DL}
Dictionary learning assumes that a data sample could be reconstructed with a linear combination of a few atoms in a dictionary.  Let $\mathbf{X}=[\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_{N}] \in \mathbb{R}^{p \times N}$ be the data matrix, where $N$ is the number of data and $p$ is the feature dimension of the data. The sparse dictionary can be learned by minimizing the following formula:
\begin{equation}\label{equ:dic_learn}
\begin{split}
&\min_{\mathbf{D,A}}\|\mathbf{X}-\mathbf{DA}\|_F^2+\lambda\|\mathbf{A}\|_1\\
&s.t. \qquad \|\mathbf{d}_i\|\leq1, \quad \forall i,\\
\end{split}
\end{equation}
where $\mathbf{D}$ is the learned dictionary and $\mathbf{d}_i$ is one of the dictionary atoms of $\mathbf{D}$. $\mathbf{A}$ is the sparse coefficient of $\mathbf{X}$, $\|\cdot\|_{1}$ denotes $l_1$-norm to enforce the sparsity, and the parameter $\lambda$ controls the sparsity. The formulation minimizes the reconstruction error of the given set of data \emph{w.r.t.} a sparsity constraint.



\subsubsection{Problem Formulation}
\label{subsec:formulation}
Assume that there are $M$-modal datasets $\{\mathbf{X}_1, \mathbf{X}_2, \dots, \mathbf{X}_M\}$. Suppose that $\mathbf{X}_m\!=\![\mathbf{x}_1^m,\mathbf{x}_2^m,\ldots,\mathbf{x}_{N}^m]\in\!\mathbb{R}^{p_m\times{N}}$ from the $m$-th modality, where $N$ denotes the number of data samples in $\mathbf{X}_m$, and $p_m$ denotes the dimensionality of the $m$-th modality. Each sample pair \{$\mathbf{x}_i^1,\ldots, \mathbf{x}_i^M$\} exclusively belongs to one of $C$ classes and describes the relevant underlying content.

Let $\mathbf{D}_m\in \mathbb{R}^{{p_m}\times{K}}$ be the dictionary from $\mathbf{X}_m$ and let $\mathbf{A}_m\in \mathbb{R}^{K \times N}$ be its corresponding sparse coefficients, where $K$ is the size of the dictionary. We also suppose that a common label matrix $\mathbf{Q}=[\mathbf{q}_1,\mathbf{q}_2,\ldots,\mathbf{q}_N]\in\!\{0,1\}^{C\times{N}}$, where $\mathbf{q}_i$ is the label vector of the $i$-th sample pair. For the $i$-th sample pair, if it belongs to the $j$-th class, $q_{ij}=1$, otherwise $q_{ij}=0$. As one sample only belongs to one class, $\sum_{j=1}^C{q_{ij}}=1$ for the $i$-th sample pair.

Our model aims to separately obtain the sparse coefficients from different modalities by dictionary learning, and project them to a common label space where the data share the similar semantic concepts by label alignment. Therefore, the object function is formulated as a minimization problem as follows:
\begin{equation}\label{equ:object}
\begin{split}
&\min_{\mathbf{D}_m,\mathbf{A}_m,\mathbf{W}_m} \sum_{m=1}^{M} \|\mathbf{X}_m-\mathbf{D}_m \mathbf{A}_m\|_F^2 +  \sum_{m=1}^M  \|\mathbf{Q}-\mathbf{W}_m \mathbf{A}_m\|_F^2 \\
&\qquad \qquad +  \sum_{m=1}^M  \lambda \|\mathbf{A}_m\|_1 +  \sum_{m=1}^M \gamma \|\mathbf{W}_m\|_F^2
\end{split}
\end{equation}
where the first term minimizes the reconstruction error, and the second term aligns the common labels of the relevant data sample by minimizing the projection error. The regularization parameters $\lambda$ and $\gamma$ can balance the weight of the two terms in the object function. In Eq. \eqref{equ:object}, $\mathbf{W}_m\in\mathbb{R}^{C \times K}$ is the projection matrix, by which $\mathbf{A}_m$ is projected into the common label space. The term ${\left\| \mathbf{W}_{m} \right\|}_{F}^2$ can avoid over-fitting. In our model, different modalities data can be represented and correlated simultaneously.


\subsubsection{Optimization}
\label{subsec:opt}

Since the objective function is not jointly convex \textit{w.r.t.} $\mathbf{D}_m$, $\mathbf{A}_m$ and $\mathbf{W}_m$, it is difficult to optimize them jointly. Therefore we use a iterative algorithm to update each variable when fixing the other two for each modal, respectively.

With the initialization of dictionaries $\mathbf{D}_m$ and projection matrices $\mathbf{W}_m$, we can update sparse coefficients $\mathbf{A}_m$ by considering $\mathbf{D}_m$ and $\mathbf{W}_m$ as constants, which can be formulated as follows:
\begin{equation}\label{equ:update-a0}
\begin{split}
\min_{\mathbf{A}_m}  {\|\mathbf{X}_m-\mathbf{D}_m \mathbf{A}_m\|_F^2 + \|\mathbf{Q}-\mathbf{W}_m \mathbf{A}_m\|_F^2 +\lambda \|\mathbf{A}_m\|_1 }
\end{split}
\end{equation}
To conduct the optimization above, it can be rewritten as the following problem:
\begin{equation}\label{equ:update-a}
\begin{split}
\min_{\mathbf{A}_m}\left\| \mathbf{\left[\begin{array}{c} \mathbf{X}_m \\ \mathbf{Q} \\ \end{array} \right]} - {\left[\begin{array}{c} \mathbf{D}_m \\  \mathbf{W}_m \\ \end{array} \right]
} \mathbf{A}_m\right\|_F^2 + \lambda \|\mathbf{A}_m\|_1
\end{split}
\end{equation}
which is a $l_1$-norm lasso problem and can be solved by SPAMS Toolbox \cite{mairal2009online}.

Then with $\mathbf{A}_m$, $\mathbf{W}_m$ fixed, we can update $\mathbf{D}_m$ as follow:
\begin{equation}\label{equ:update-d}
\begin{split}
\min_{\mathbf{D}_m} \|\mathbf{X}_m - \mathbf{D}_m \mathbf{A}_m\|_F^2
\quad \mbox{s.t.} \quad \|\mathbf{d}_{m,i}\| \leq 1, \forall i
 \end{split}
\end{equation}
which is a quadratically constrained quadratic program (QCQP) problem \emph{w.r.t.} $\mathbf{D}_{m}$ and the solutions can be obtained by Lagrange dual techniques \cite{lee2006efficient}.

When $\mathbf{A}_m$ and $\mathbf{D}_m$ are fixed, we can calculate projection matrices $\mathbf{W}_m$ as follows:
\begin{equation}\label{equ:update-w0}
\min_{\mathbf{W}_m} { \|\mathbf{Q}-\mathbf{W}_m \mathbf{A}_m\|_F^2 + \gamma \|\mathbf{W}_m\|_F^2}
\end{equation}


Finally, we update the projection matrices $\mathbf{U}_m$. When $\mathbf{D}_m$ and $\mathbf{A}_m$ are fixed, Eq. \eqref{equ:object} is a ridge regression problem and the analytical solutions can be obtained as below:
\begin{equation}\label{equ:update-w}
\mathbf{W}_m = \mathbf{Q} \mathbf{A}_m^T(\mathbf{A}_m \mathbf{A}_m^T + \gamma \mathbf{I})^{-1}
\end{equation}

The algorithm procedures are summarized in Algorithm \ref{alg:optimization}.


\renewcommand{\algorithmicrequire}{\textbf{Input:}} % Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm
\begin{algorithm}[!t]
\caption{Iterative Algorithm for Our Proposed Model}
\label{alg:optimization}
\begin{algorithmic}[1]
\Require
Data matrices $\mathbf{X}_m$ from the $m$-th modality; the label matrix $\mathbf{Q}$.
\State Initialize the dictionary $\mathbf{D}_m$ and the sparse coefficients $\mathbf{A}_m$ for $m$-th modality and initialize the projection matrix $\mathbf{W}_m$ by Eq. \eqref{equ:update-w}.
\While {not converge}
\State Update $\mathbf{A}_{m}$ of the $m$-th modality by Eq. \eqref{equ:update-a} with $\mathbf{D}_{m}$ and $\mathbf{W}_{m}$ obtained from the previous iteration.
\State Update $\mathbf{D}_{m}$ of the $m$-th modality by Eq. \eqref{equ:update-d} with $\mathbf{A}_{m}$ and $\mathbf{W}_{m}$ fixed.
\State Update $\mathbf{W}_{m}$ of the $m$-th modality by Eq. \eqref{equ:update-w} with $\mathbf{D}_{m}$ and $\mathbf{A}_{m}$ fixed.
\EndWhile
\Ensure
Dictionaries $\mathbf{D}_m$, and project matrices $\mathbf{W}_m$ of data from the $m$-th modality.
\end{algorithmic}
\end{algorithm}



\subsection{Application to Cross-Modal Retrieval}
\label{subsec:retrieval}
After the optimization algorithm is completed, $\mathbf{D}_m$, $\mathbf{A}_m$ and $\mathbf{U}_m$ can be derived and we can use them to conduct cross-modal retrieval application.

Given a query $q_m$ from $m$-th modality, the sparse coefficients $a_m$ can be obtained by sparse coding as follows:
\begin{equation}\label{equ:obtain_a_m}
\min_{a_m}{\|q_m-\mathbf{D}_i a_m\|_F^2+\lambda\|a_m\|_1}
\end{equation}
Then the sparse coefficients will be projected into the common label space by the projection matrix $\mathbf{W}_m$:
\begin{equation}\label{equ:obtain_p_m}
p_m = \mathbf{W}_m a_m
\end{equation}
where $p_m$ is the representation of $q_m$ in the common label space. By using the projection matrices we can projected all data from different modalities into the the common label space, in which the relevance of data from different modalities can be measured easily and the nearest neighbors of the query are returned as the retrieval results.


\subsection{Experiments}
\label{sec:experiments}

In this section, we will evaluate the performance of the proposed cross-modal retrieval. We first elaborate the experiment setting and the evaluation metrics adopted in this paper, and then compare our approach with several state-of-the-art methods.

\subsubsection{Experiment Setting}
\label{subsec:setup}
We evaluate the proposed method on two public image-text datasets, \emph{i.e.}, Wiki Text-Image dataset \cite{rasiwasia2010new} and the NUS-WIDE dataset \cite{chua2009nus}. And our experiments are conducted for two retrieval tasks: (1) image query in text database, (2) text query in image database.

The Wiki dataset consists of 2173/693 (training/test) image-text pairs which are generated from Wikipedia. Each pair is labeled by only one of 10 semantic classes. SIFT descriptors of images are extracted and quantized into Bag-of-Visual-Words (BoVW) by K-means clustering. For texts, we represent them with Bag-of-Words (BoW) by counting the word frequency. Considering the effect of feature dimensions, we finally obtain two datasets: one with 500 dimensions BoVW and 1,000 dimensions BoW, and the other with 1,000 dimensions BoVW and 5,000 dimensions BoW.

The NUS-WIDE dataset contains 269,648 images from Flickr, with a total number of 5,018 unique tags. It contains 81 labels and some image-tags pairs have two or more labels. In our experiments, image-tags pairs with only one label are selected. We use 500 dimensions BoVW based on SIFT as image features and 1000 dimensions tags as text feature. Finally 63,641 pairs are obtained as dataset. And we randomly take 3\% each class for training set and 1\% each class from the testing set.

In the paper, mean Average Precision (mAP) is utilized to evaluate the performance and the cosine distance is adopt to measure the similarity. The Average Precision (AP) is defined as $AP=\frac{1}{T}\sum^{N}_{r=1}P(r)\delta(r)$, where $T$ is the number of retrieved data belonging to the same class of the query. $P(r)$ denotes the precision of the top $r$ retrieved data, and $\delta(r)=1$ if the $r$th retrieved data has the same label as the query and $\delta(r)=0$ otherwise. In the experiments, we set $N=50$. The average AP values over all the queries in the query set is obtained as mAP.
In our experiments, both the parameters $\lambda$ and $\gamma$ are set to 0.1,  and dictionary size $K=200$ empirically.



\begin{table}[t]
\centering
\caption{Comparison of mAP performance of different methods on the Wiki dataset with 500-D BoVW and 1000-D BoW, for image query text task, text query image task and the average of two tasks. The best results are marked by bold font.}
\begin{tabular}{lccc}
%\hline\noalign{\smallskip}
%\multicolumn{4}{c}{500-D BoVW and 1000-D BoW}\\
\hline\noalign{\smallskip}
\multirow{2}{*}{Methods} & \multicolumn{3}{c}{mAP} \\
\cline{2-4}\noalign{\smallskip}
 & Image query& Text query& Average\\
\hline\noalign{\smallskip}
CCA  & 0.1785	& 0.1784 &	0.1785\\
PLS  & 0.2850	& 0.3419 &	0.3135\\
BLM  & 0.2575	& 0.2806 &	0.2691\\
GMLDA & 0.2770	& 0.3411 &	0.3091\\
GMMFA & 0.2722	& 0.3241 &	0.2982\\
SLiM$^2$ & 0.2242	& 0.2334 &	0.2288\\
Proposed & \textbf{0.3094} & \textbf{0.3762} & \textbf{0.3428}\\
\hline
\end{tabular}
\label{tab:wiki-small}
\end{table}


\begin{table}[t]
\centering
\caption{Comparison of mAP performance of different methods on the Wiki dataset with 1000-D BoVW and 5000-D BoW, for image query text task, text query image task and the average of two tasks. The best results are marked by bold font.}
\begin{tabular}{lccc}
%\hline\noalign{\smallskip}
%\multicolumn{4}{c}{1000-D BoVW and 5000-D BoW}\\
\hline\noalign{\smallskip}
\multirow{2}{*}{Methods} & \multicolumn{3}{c}{mAP} \\
\cline{2-4}\noalign{\smallskip}
 & Image query& Text query& Average\\
\hline\noalign{\smallskip}
CCA	 & 0.2454 & 0.2405 & 0.2430 \\
PLS & 0.2892 & 0.3258 & 0.3075 \\
BLM & 0.2623 & 0.3247 & 0.2935 \\
GMLDA & 0.2456 & 0.2259 & 0.2358 \\
GMMFA & 0.2020 & 0.2449 & 0.2235 \\
SLiM$^2$ & 0.2647 & 0.2852 & 0.2750 \\
Proposed & \textbf{0.3410} & \textbf{0.4253} & \textbf{0.3832}\\
\hline
\end{tabular}
\label{tab:wiki-large}
\end{table}




\begin{table}[t]
\centering
\caption{Comparison of mAP performance of different methods on the NUS-WIDE dataset with 500-D BoVW and 1000-D BoW for image query text task, text query image task and the average of two tasks. The best results are marked by bold font.}\label{tab:nuswide}
\begin{tabular}{lccc}
%\hline\noalign{\smallskip}
%\multicolumn{4}{c}{500-D BoVW and 1000-D tags}\\
\hline\noalign{\smallskip}
\multirow{2}{*}{Methods} & \multicolumn{3}{c}{mAP} \\
\cline{2-4}\noalign{\smallskip}
 & Image query& Text query& Average\\
\hline\noalign{\smallskip}
CCA & 0.3329 & 0.3393 & 0.3361 \\
PLS & \textbf{0.4297} & 0.4363 & 0.4330 \\
BLM	& 0.4264 & 0.4249 & 0.4257 \\
GMLDA &	0.3775 & 0.4138	& 0.3957 \\
GMMFA &	0.3795 & 0.4067	& 0.3931 \\
SLiM$^2$ &	0.3652 & 0.3500	& 0.3576 \\
Proposed & 0.4217 & \textbf{0.4903} & \textbf{0.4560} \\
\hline
\end{tabular}
\end{table}

\subsubsection{Performance Comparisons}
\label{subsec:performance}

We evaluate the performance of our proposed method, and compare it with several related methods: CCA \cite{hardoon2004canonical}, PLS \cite{sharma2011bypassing},  BLM \cite{tenenbaum2000separating},  GMMFA \cite{sharma2012generalized},  GMLDA \cite{sharma2012generalized}, and SLiM$^2$ \cite{zhuang2013supervised} over Wiki Text-Image dataset and NUS-WIDE dataset.


Table \ref{tab:wiki-small} and Table \ref{tab:wiki-large} show the mAP scores of different methods on the Wiki dataset. As shown in Table \ref{tab:wiki-small} and Table \ref{tab:wiki-large}, our method outperforms other approaches in both two retrieval tasks on Wiki dataset. Moreover, we obtain the highest average mAP scores of 0.3428 and 0.3832 on two dataset, respectively. The reason is that our method not only effectively represent heterogenous features by sparse coefficients and also sufficiently explore the common label information.

The comparison of mAP results on NUS-WIDE dataset are shown in Table \ref{tab:nuswide}. As we have seen, our method achieves mAP score of 0.4903 on image-query-text task, which is much better than other methods. Although the PLS method outperforms slightly on text-query-image task, our method obtains the best mAP score on average.

Figure \ref{fig:example} illustrates a visual case of cross-modal retrieval by our method. The top row shows the result of text-query-images task and the bottom row is the image-query-texts task. It is worth noting that our method can retrieve the semantically related results.


\begin{figure*}[!t]
\includegraphics[width=\linewidth]{./fig/example4.eps}
%\makeatletter\def\@captype{figure}\makeatother
\caption{Top: An example of an text query and the top four images retrieved by our method.  Bottom: An example of an image query and the corresponding images of the top four texts retrieved by our method.}
\label{fig:example}
\end{figure*}

\section{Conclusions}
\label{sec:conclusion}
In this paper, we propose a cross-modal retrieval method based on coupled dictionary learning
with common label alignment. The main contributions of our method include: (1) we exploit coupled dictionary learning to project the data from different modalities into a common space where the inherent relation between modalities can be well discovered; (2) we utilize the shared label information in the common space to further encourage the correlation across the different modalities. Experiment results on two public datasets confirm that our method outperforms several state-of-the-art approaches.
